---
title: "Download content"
description: "Download PDFs and machine-readable XML for OpenAlex works"
---

OpenAlex includes links to publisher-hosted and repository-hosted full text for about 60 million [Open Access](/api-reference/entities/works#open_access) works. But downloading from all those different sources can be inconvenient.

So we've cached copies of these files:

| Format | Count | Description |
|--------|-------|-------------|
| **PDF** | 60M | Original PDF files |
| **TEI XML** | 43M | Parsed PDFs (via [Grobid](https://github.com/kermitt2/grobid)) in [TEI XML](https://tei-c.org/) format |
| **Markdown** | â€” | Coming soon |

<Warning>
Content downloads require an API key and cost **100 credits per file**. See [rate limits](/api-reference/rate-limits) for details.
</Warning>

## Download a single work

The URL pattern is simple:

```
https://content.openalex.org/works/{work_id}.pdf?api_key=YOUR_KEY
```

Replace `{work_id}` with any OpenAlex work ID (like `W2741809807`). Use `.grobid-xml` instead of `.pdf` to get the TEI XML version.

**Examples:**

```bash
# Download PDF
curl "https://content.openalex.org/works/W2741809807.pdf?api_key=YOUR_KEY" -o paper.pdf

# Download TEI XML
curl "https://content.openalex.org/works/W2741809807.grobid-xml?api_key=YOUR_KEY" -o paper.xml
```

## Download multiple works

For a list of work IDs, iterate through them:

```python
import requests

work_ids = ["W2741809807", "W4388482763", "W4386073691"]

for work_id in work_ids:
    r = requests.get(
        f"https://content.openalex.org/works/{work_id}.pdf",
        params={"api_key": "YOUR_KEY"},
        allow_redirects=True
    )
    with open(f"{work_id}.pdf", "wb") as f:
        f.write(r.content)
```

For higher volume (thousands to millions of downloads), use our command-line tool:

```bash
pip install openalex-content-downloader

openalex-content download \
  --api-key YOUR_KEY \
  --output ./pdfs \
  --filter "has_content.pdf:true"
```

The tool handles parallel downloads, automatic retries, and checkpointing so you can resume interrupted downloads.

## Find works with content

### Check the work object

If you already have a work object, look for the `content_url` field:

```json
{
  "id": "https://openalex.org/W2741809807",
  "content_url": "https://content.openalex.org/works/W2741809807"
}
```

If present, append `.pdf` or `.grobid-xml` and add your API key.

### Use the has_content filter

Use the `has_content` filter to find works with downloadable content:

```
https://api.openalex.org/works?filter=default.search:frogs,has_content.pdf:true
```

Or combine with other filters:

```
https://api.openalex.org/works?filter=has_content.pdf:true,best_oa_location.license:cc-by,publication_year:2024-
```

## How it works

When you request content:

1. We check if we have the file. If not, you get a `404`.
2. We verify your API key has enough credits.
3. We generate a [presigned URL](https://developers.cloudflare.com/r2/api/s3/presigned-urls/) for [Cloudflare R2](https://developers.cloudflare.com/r2/).
4. We return a `302 redirect` to that URL. Your client follows it automatically.
5. Cloudflare serves the file from their global edge network.

The presigned URL expires after 5 minutes. Request again for a fresh URL (costs another 100 credits).

## Example: Build a corpus for AI synthesis

Find works about microplastics with PDFs:

```
GET https://api.openalex.org/works?filter=default.search:microplastics%20drinking%20water,has_content.pdf:true&select=id,title,content_url&per_page=100
```

Download and convert to text:

```python
import requests
import subprocess

work_ids = ["W4388482763", "W4386073691"]  # from API results

for work_id in work_ids:
    r = requests.get(
        f"https://content.openalex.org/works/{work_id}.pdf",
        params={"api_key": "YOUR_KEY"},
        allow_redirects=True
    )

    with open(f"{work_id}.pdf", "wb") as f:
        f.write(r.content)

    # Convert to markdown (using marker, pdftotext, or similar)
    subprocess.run(["marker", f"{work_id}.pdf", "-o", f"{work_id}.md"])
```

<Tip>
For bulk downloads, use the CLI: `openalex-content download --filter "default.search:microplastics,has_content.pdf:true"`
</Tip>

## Credit costs

| Action | Credits |
|--------|---------|
| Get a work by ID | 0 |
| List/filter works | 1 |
| Download PDF or TEI XML | 100 |
